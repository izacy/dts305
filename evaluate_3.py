import os
import re
import csv
import math
import numpy as np
from datetime import datetime
from collections import defaultdict, Counter
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.util import ngrams

# å°è¯•ä¸‹è½½æ‰€éœ€çš„NLTKæ•°æ®
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    print("ä¸‹è½½NLTKæ•°æ®...")
    nltk.download('punkt')
    nltk.download('stopwords')

from main_3 import InformationRetrievalSystem


class DocumentBasedQueryGenerator:
    """
    åŸºäºæ–‡æ¡£å†…å®¹çš„æŸ¥è¯¢ç”Ÿæˆå™¨
    """
    
    def __init__(self):
        self.irs = InformationRetrievalSystem()
        self.stop_words = set(stopwords.words('english'))
        
        # ä¸ºæ›´å¥½çš„æŸ¥è¯¢è´¨é‡æ·»åŠ é¢å¤–çš„åœç”¨è¯
        self.stop_words.update(['said', 'also', 'would', 'could', 'one', 'two', 
                               'first', 'last', 'new', 'old', 'good', 'well',
                               'way', 'much', 'many', 'take', 'make', 'get'])
    
    def extract_key_phrases_from_document(self, doc_content, doc_name, max_phrases=10):
        """
        ä»æ–‡æ¡£ä¸­æå–æœ‰æ„ä¹‰çš„çŸ­è¯­
        
        Args:
            doc_content (str): æ–‡æ¡£æ–‡æœ¬
            doc_name (str): æ–‡æ¡£æ–‡ä»¶å
            max_phrases (int): æœ€å¤šæå–çš„çŸ­è¯­æ•°
            
        Returns:
            list: å…³é”®çŸ­è¯­åˆ—è¡¨ï¼ŒåŒ…å«å…ƒæ•°æ®
        """
        # æ¸…ç†å’Œåˆ†è¯
        sentences = sent_tokenize(doc_content)
        words = word_tokenize(doc_content.lower())
        
        # ç§»é™¤åœç”¨è¯å’ŒçŸ­è¯
        meaningful_words = [word for word in words 
                           if word.isalpha() and len(word) > 3 
                           and word not in self.stop_words]
        
        # è·å–è¯é¢‘
        word_freq = Counter(meaningful_words)
        
        # æå–ä¸åŒç±»å‹çš„çŸ­è¯­
        phrases = []
        
        # 1. é«˜é¢‘æœ‰æ„ä¹‰è¯æ±‡ (å•è¯æŸ¥è¯¢)
        for word, freq in word_freq.most_common(5):
            if freq >= 2:  # è¯æ±‡è‡³å°‘å‡ºç°ä¸¤æ¬¡
                phrases.append({
                    'phrase': word,
                    'type': 'single_word',
                    'frequency': freq,
                    'source_doc': doc_name,
                    'length': 1
                })
        
        # 2. æå–åè¯çŸ­è¯­å’Œæœ‰æ„ä¹‰çš„åŒè¯ç»„åˆ
        pos_tagged = nltk.pos_tag(meaningful_words)
        
        # ç®€å•åè¯çŸ­è¯­æå– (åè¯ + åè¯, å½¢å®¹è¯ + åè¯)
        for i in range(len(pos_tagged) - 1):
            word1, pos1 = pos_tagged[i]
            word2, pos2 = pos_tagged[i + 1]
            
            # åè¯ + åè¯ æˆ– å½¢å®¹è¯ + åè¯
            if ((pos1.startswith('NN') and pos2.startswith('NN')) or
                (pos1.startswith('JJ') and pos2.startswith('NN'))):
                
                bigram = f"{word1} {word2}"
                
                # æ£€æŸ¥æ­¤åŒè¯ç»„åˆæ˜¯å¦åœ¨åŸæ–‡ä¸­å‡ºç°
                if bigram in doc_content.lower():
                    phrases.append({
                        'phrase': bigram,
                        'type': 'noun_phrase',
                        'frequency': doc_content.lower().count(bigram),
                        'source_doc': doc_name,
                        'length': 2
                    })
        
        # 3. ä»å¥å­ä¸­æå–æœ‰æ„ä¹‰çš„ä¸‰å…ƒç»„
        for sentence in sentences[:5]:  # æ£€æŸ¥å‰5ä¸ªå¥å­
            sentence_words = [word.lower() for word in word_tokenize(sentence) 
                            if word.isalpha() and word.lower() not in self.stop_words]
            
            if len(sentence_words) >= 3:
                for trigram in ngrams(sentence_words, 3):
                    trigram_phrase = ' '.join(trigram)
                    
                    # åªåŒ…å«æœ‰æ„ä¹‰å†…å®¹
                    if any(word in word_freq and word_freq[word] >= 2 for word in trigram):
                        phrases.append({
                            'phrase': trigram_phrase,
                            'type': 'sentence_trigram',
                            'frequency': 1,
                            'source_doc': doc_name,
                            'length': 3
                        })
        
        # 4. æå–å…³é”®å¥å­ (ç®€åŒ–ç‰ˆ)
        for sentence in sentences:
            words_in_sentence = [word.lower() for word in word_tokenize(sentence) 
                               if word.isalpha()]
            
            # é€šè¿‡é‡è¦è¯æ±‡é¢‘ç‡ä¸ºå¥å­è¯„åˆ†
            sentence_score = sum(word_freq.get(word, 0) for word in words_in_sentence)
            
            if sentence_score >= 5 and len(words_in_sentence) <= 8:
                # ä»å¥å­ä¸­çš„é‡è¦è¯æ±‡åˆ›å»ºçŸ­è¯­
                important_words = [word for word in words_in_sentence 
                                 if word in word_freq and word_freq[word] >= 2][:4]
                
                if len(important_words) >= 2:
                    phrase = ' '.join(important_words)
                    phrases.append({
                        'phrase': phrase,
                        'type': 'key_sentence_extract',
                        'frequency': sentence_score,
                        'source_doc': doc_name,
                        'length': len(important_words)
                    })
        
        # ç§»é™¤é‡å¤å¹¶æŒ‰ç›¸å…³æ€§æ’åº
        unique_phrases = {}
        for phrase_info in phrases:
            phrase = phrase_info['phrase']
            if phrase not in unique_phrases:
                unique_phrases[phrase] = phrase_info
            elif phrase_info['frequency'] > unique_phrases[phrase]['frequency']:
                unique_phrases[phrase] = phrase_info
        
        # æŒ‰é¢‘ç‡å’Œé•¿åº¦æ’åº (åå¥½æ›´å…·ä½“çš„çŸ­è¯­)
        sorted_phrases = sorted(unique_phrases.values(), 
                              key=lambda x: (x['frequency'] * x['length']), 
                              reverse=True)
        
        return sorted_phrases[:max_phrases]
    
    def generate_document_based_queries(self):
        """
        ä»é›†åˆä¸­çš„æ‰€æœ‰æ–‡æ¡£ç”ŸæˆæŸ¥è¯¢
        
        Returns:
            list: ç”Ÿæˆçš„æŸ¥è¯¢åˆ—è¡¨ï¼ŒåŒ…å«å…ƒæ•°æ®
        """
        print("ğŸ“š ä»æ–‡æ¡£å†…å®¹ç”ŸæˆæŸ¥è¯¢")
        print("=" * 60)
        
        # åˆå§‹åŒ–IRç³»ç»Ÿä»¥åŠ è½½æ–‡æ¡£
        if not self.irs.initialize_system():
            print("æ–‡æ¡£åŠ è½½å¤±è´¥")
            return []
        
        all_queries = []
        query_id = 1
        
        # ä»æ¯ä¸ªæ–‡æ¡£æå–çŸ­è¯­
        for i, (doc_name, doc_content) in enumerate(zip(self.irs.document_names, self.irs.documents)):
            print(f"\nğŸ“„ åˆ†æ {doc_name}:")
            
            # æå–å…³é”®çŸ­è¯­
            phrases = self.extract_key_phrases_from_document(doc_content, doc_name, max_phrases=5)
            
            print(f"   æå–äº† {len(phrases)} ä¸ªå…³é”®çŸ­è¯­:")
            
            # å°†çŸ­è¯­è½¬æ¢ä¸ºæŸ¥è¯¢
            for phrase_info in phrases[:3]:  # æ¯ä¸ªæ–‡æ¡£ä½¿ç”¨å‰3ä¸ªçŸ­è¯­
                query_info = {
                    'id': f'Q{query_id}',
                    'query': phrase_info['phrase'],
                    'type': phrase_info['type'],
                    'length': phrase_info['length'],
                    'source_document': doc_name,
                    'source_doc_index': i,
                    'frequency_in_source': phrase_info['frequency'],
                    'expected_category': doc_name.split('_')[0] if '_' in doc_name else 'main'
                }
                
                all_queries.append(query_info)
                print(f"   Q{query_id}: '{phrase_info['phrase']}' ({phrase_info['type']}, freq={phrase_info['frequency']})")
                
                query_id += 1
        
        print(f"\nâœ… ä»æ–‡æ¡£å†…å®¹ç”Ÿæˆäº† {len(all_queries)} ä¸ªæŸ¥è¯¢")
        
        return all_queries
    
    def create_automatic_relevance_judgments(self, queries):
        """
        åŸºäºæŸ¥è¯¢æ¥æºå’Œå†…å®¹åˆ†æè‡ªåŠ¨åˆ›å»ºç›¸å…³æ€§åˆ¤æ–­
        
        Args:
            queries (list): æŸ¥è¯¢å­—å…¸åˆ—è¡¨
            
        Returns:
            dict: æ¯ä¸ªæŸ¥è¯¢çš„ç›¸å…³æ€§åˆ¤æ–­
        """
        print(f"\nğŸ¯ åˆ›å»ºè‡ªåŠ¨ç›¸å…³æ€§åˆ¤æ–­")
        print("-" * 50)
        
        relevance_judgments = {}
        
        for query_info in queries:
            query_id = query_info['id']
            query_text = query_info['query']
            source_doc_index = query_info['source_doc_index']
            expected_category = query_info['expected_category']
            
            # åˆå§‹åŒ–ç›¸å…³æ€§åˆ†æ•°
            relevance = {}
            
            for doc_idx in range(len(self.irs.documents)):
                doc_name = self.irs.document_names[doc_idx]
                doc_content = self.irs.documents[doc_idx].lower()
                doc_category = doc_name.split('_')[0] if '_' in doc_name else 'main'
                
                # åŸºç¡€ç›¸å…³æ€§è®¡ç®—
                relevance_score = 0
                
                # 1. æ¥æºæ–‡æ¡£è·å¾—æœ€é«˜ç›¸å…³æ€§
                if doc_idx == source_doc_index:
                    relevance_score = 3  # é«˜åº¦ç›¸å…³
                
                # 2. åŒç±»åˆ«æ–‡æ¡£
                elif doc_category == expected_category:
                    # æ£€æŸ¥æŸ¥è¯¢è¯æ±‡æ˜¯å¦åœ¨æ–‡æ¡£ä¸­å‡ºç°
                    query_words = query_text.lower().split()
                    word_matches = sum(1 for word in query_words if word in doc_content)
                    
                    if word_matches == len(query_words):
                        relevance_score = 2  # ç›¸å…³
                    elif word_matches > 0:
                        relevance_score = 1  # éƒ¨åˆ†ç›¸å…³
                    else:
                        relevance_score = 0  # ä¸ç›¸å…³
                
                # 3. ä¸åŒç±»åˆ«æ–‡æ¡£
                else:
                    # æ£€æŸ¥è¯­ä¹‰ç›¸å…³æ€§ (æŸ¥è¯¢è¯æ±‡åœ¨å†…å®¹ä¸­)
                    query_words = query_text.lower().split()
                    word_matches = sum(1 for word in query_words if word in doc_content)
                    
                    if word_matches == len(query_words):
                        relevance_score = 1  # éƒ¨åˆ†ç›¸å…³ (è·¨é¢†åŸŸ)
                    elif word_matches > len(query_words) / 2:
                        relevance_score = 1  # æŸäº›ç›¸å…³æ€§
                    else:
                        relevance_score = 0  # ä¸ç›¸å…³
                
                relevance[doc_idx] = relevance_score
            
            relevance_judgments[query_id] = relevance
            
            # æ˜¾ç¤ºç›¸å…³æ€§åˆ¤æ–­æ‘˜è¦
            relevant_docs = sum(1 for score in relevance.values() if score > 0)
            highly_relevant = sum(1 for score in relevance.values() if score >= 2)
            
            print(f"   {query_id}: '{query_text}' - {relevant_docs} ç›¸å…³æ–‡æ¡£ ({highly_relevant} é«˜åº¦ç›¸å…³)")
        
        return relevance_judgments
    
    def select_diverse_queries(self, all_queries, target_count=5):
        """
        é€‰æ‹©å¤šæ ·åŒ–çš„æŸ¥è¯¢é›†åˆç”¨äºè¯„ä¼°
        
        Args:
            all_queries (list): æ‰€æœ‰ç”Ÿæˆçš„æŸ¥è¯¢
            target_count (int): è¦é€‰æ‹©çš„æŸ¥è¯¢æ•°
            
        Returns:
            list: é€‰æ‹©çš„å¤šæ ·åŒ–æŸ¥è¯¢
        """
        print(f"\nğŸ² é€‰æ‹© {target_count} ä¸ªå¤šæ ·åŒ–æŸ¥è¯¢")
        print("-" * 40)
        
        # æŒ‰ç‰¹å¾åˆ†ç»„æŸ¥è¯¢
        by_category = defaultdict(list)
        by_length = defaultdict(list)
        by_type = defaultdict(list)
        
        for query in all_queries:
            by_category[query['expected_category']].append(query)
            by_length[query['length']].append(query)
            by_type[query['type']].append(query)
        
        selected_queries = []
        
        # ç­–ç•¥ï¼šé€‰æ‹©æŸ¥è¯¢ä»¥æœ€å¤§åŒ–å¤šæ ·æ€§
        
        # 1. ç¡®ä¿ç±»åˆ«å¤šæ ·æ€§
        categories = list(by_category.keys())
        queries_per_category = target_count // len(categories)
        remainder = target_count % len(categories)
        
        for i, category in enumerate(categories):
            count = queries_per_category + (1 if i < remainder else 0)
            
            # ä»æ­¤ç±»åˆ«é€‰æ‹©æœ€ä½³æŸ¥è¯¢ (æŒ‰é¢‘ç‡å’Œé•¿åº¦)
            category_queries = sorted(by_category[category], 
                                    key=lambda x: (x['frequency_in_source'] * x['length']), 
                                    reverse=True)
            
            selected_queries.extend(category_queries[:count])
        
        # 2. å¦‚æœä»éœ€è¦æ›´å¤šæŸ¥è¯¢ï¼ŒæŒ‰å¤šæ ·æ€§é€‰æ‹©
        while len(selected_queries) < target_count:
            remaining_queries = [q for q in all_queries if q not in selected_queries]
            if not remaining_queries:
                break
            
            # é€‰æ‹©å…·æœ‰æœ€ä¸åŒç‰¹å¾çš„æŸ¥è¯¢
            best_query = max(remaining_queries, 
                           key=lambda x: (x['length'] * 2 + x['frequency_in_source']))
            selected_queries.append(best_query)
        
        # ä¸ºä¸€è‡´æ€§é‡æ–°åˆ†é…æŸ¥è¯¢ID
        for i, query in enumerate(selected_queries, 1):
            query['id'] = f'Q{i}'
        
        print("é€‰æ‹©çš„æŸ¥è¯¢:")
        for query in selected_queries:
            print(f"   {query['id']}: '{query['query']}' (æ¥è‡ª {query['source_document']}, {query['type']})")
        
        return selected_queries[:target_count]


class DocumentBasedEvaluator:
    """
    ä½¿ç”¨åŸºäºæ–‡æ¡£çš„æŸ¥è¯¢çš„å®Œæ•´è¯„ä¼°å™¨
    """
    
    def __init__(self):
        self.query_generator = DocumentBasedQueryGenerator()
        self.irs = self.query_generator.irs
        self.selected_queries = []
        self.relevance_judgments = {}
        
    def calculate_precision_at_k(self, retrieved_docs, relevant_docs, k=5):
        """è®¡ç®—Precision@K"""
        if not retrieved_docs or k == 0:
            return 0.0
        
        top_k_docs = [doc_id for doc_id, _ in retrieved_docs[:k]]
        relevant_retrieved = sum(1 for doc_id in top_k_docs 
                               if relevant_docs.get(doc_id, 0) > 0)
        
        return relevant_retrieved / k
    
    def calculate_recall_at_k(self, retrieved_docs, relevant_docs, k=5):
        """è®¡ç®—Recall@K"""
        if not retrieved_docs or k == 0:
            return 0.0
        
        total_relevant = sum(1 for rel in relevant_docs.values() if rel > 0)
        if total_relevant == 0:
            return 0.0
        
        top_k_docs = [doc_id for doc_id, _ in retrieved_docs[:k]]
        relevant_retrieved = sum(1 for doc_id in top_k_docs 
                               if relevant_docs.get(doc_id, 0) > 0)
        
        return relevant_retrieved / total_relevant
    
    def calculate_dcg_at_k(self, retrieved_docs, relevant_docs, k=5):
        """è®¡ç®—DCG@K"""
        if not retrieved_docs or k == 0:
            return 0.0
        
        dcg = 0.0
        for i, (doc_id, _) in enumerate(retrieved_docs[:k]):
            relevance = relevant_docs.get(doc_id, 0)
            
            if i == 0:
                dcg += relevance
            else:
                dcg += relevance / math.log2(i + 2)
        
        return dcg
    
    def calculate_ideal_dcg_at_k(self, relevant_docs, k=5):
        """è®¡ç®—Ideal DCG@K"""
        sorted_relevances = sorted(relevant_docs.values(), reverse=True)
        
        idcg = 0.0
        for i, relevance in enumerate(sorted_relevances[:k]):
            if i == 0:
                idcg += relevance
            else:
                idcg += relevance / math.log2(i + 2)
        
        return idcg
    
    def calculate_ndcg_at_k(self, retrieved_docs, relevant_docs, k=5):
        """è®¡ç®—nDCG@K"""
        dcg = self.calculate_dcg_at_k(retrieved_docs, relevant_docs, k)
        idcg = self.calculate_ideal_dcg_at_k(relevant_docs, k)
        
        if idcg == 0:
            return 0.0
        
        return dcg / idcg
    
    def calculate_fusion_results(self, lsi_results, lm_results, bm25_results, weights=[0.25, 0.35, 0.4]):
        """è®¡ç®—èåˆç»“æœ"""
        def normalize_scores(results):
            if not results:
                return {}
            scores = [score for _, score in results]
            min_score, max_score = min(scores), max(scores)
            if max_score == min_score:
                return {doc_idx: 0.5 for doc_idx, _ in results}
            return {doc_idx: (score - min_score) / (max_score - min_score) 
                   for doc_idx, score in results}
        
        lsi_scores = normalize_scores(lsi_results)
        lm_scores = normalize_scores(lm_results)
        bm25_scores = normalize_scores(bm25_results)
        all_doc_indices = set(lsi_scores.keys()) | set(lm_scores.keys()) | set(bm25_scores.keys())
        
        fusion_results = []
        for doc_idx in all_doc_indices:
            lsi_score = lsi_scores.get(doc_idx, 0)
            lm_score = lm_scores.get(doc_idx, 0)
            bm25_score = bm25_scores.get(doc_idx, 0)
            fusion_score = (weights[0] * lsi_score + 
                           weights[1] * lm_score + 
                           weights[2] * bm25_score)
            fusion_results.append((doc_idx, fusion_score))
        
        fusion_results.sort(key=lambda x: x[1], reverse=True)
        return fusion_results
    
    def evaluate_single_query(self, query_info):
        """è¯„ä¼°å•ä¸ªæŸ¥è¯¢"""
        query_id = query_info['id']
        query_text = query_info['query']
        
        print(f"\nğŸ” è¯„ä¼° {query_id}: '{query_text}' (æ¥è‡ª {query_info['source_document']})")
        
        # ä»æ‰€æœ‰ä¸‰ç§æ–¹æ³•è·å–ç»“æœ
        lsi_results = self.irs.information_retrieval_algorithm_1(query_text, top_k=5)
        lm_results = self.irs.information_retrieval_algorithm_2(query_text, top_k=5)
        bm25_results = self.irs.information_retrieval_algorithm_3(query_text, top_k=5)
        fusion_results = self.calculate_fusion_results(lsi_results, lm_results, bm25_results)
        
        # è·å–ç›¸å…³æ€§åˆ¤æ–­
        relevant_docs = self.relevance_judgments[query_id]
        
        # è®¡ç®—æŒ‡æ ‡
        result = {
            'query_id': query_id,
            'query_text': query_text,
            'query_type': query_info['type'],
            'source_document': query_info['source_document'],
            'expected_category': query_info['expected_category']
        }
        
        methods = [
            ('LSI', lsi_results),
            ('Language_Model', lm_results),
            ('BM25', bm25_results),
            ('Fusion', fusion_results)
        ]
        
        for method_name, retrieved_docs in methods:
            precision = self.calculate_precision_at_k(retrieved_docs, relevant_docs, k=5)
            recall = self.calculate_recall_at_k(retrieved_docs, relevant_docs, k=5)
            ndcg = self.calculate_ndcg_at_k(retrieved_docs, relevant_docs, k=5)
            
            result[f'{method_name}_Precision@5'] = precision
            result[f'{method_name}_Recall@5'] = recall
            result[f'{method_name}_nDCG@5'] = ndcg
            
            print(f"   {method_name:12}: P@5={precision:.3f}, R@5={recall:.3f}, nDCG@5={ndcg:.3f}")
            
            # æ˜¾ç¤ºè¢«æ£€ç´¢çš„å†…å®¹
            if retrieved_docs:
                top_3 = [(self.irs.document_names[doc_id], score, relevant_docs.get(doc_id, 0)) 
                        for doc_id, score in retrieved_docs[:3]]
                print(f"      å‰3å: {[(name.split('_')[0], f'{score:.3f}', f'rel={rel}') for name, score, rel in top_3]}")
        
        return result
    
    def run_document_based_evaluation(self):
        """è¿è¡Œå®Œæ•´çš„åŸºäºæ–‡æ¡£çš„è¯„ä¼°"""
        print("ğŸ“„ åŸºäºæ–‡æ¡£çš„æŸ¥è¯¢è¯„ä¼°")
        print("DTS305TC Natural Language Processing - Part 4")
        print("=" * 80)
        
        print("ğŸ¯ æ–¹æ³•:")
        print("  â€¢ ç›´æ¥ä»æ–‡æ¡£å†…å®¹ç”ŸæˆæŸ¥è¯¢")
        print("  â€¢ åŸºäºå†…å®¹åˆ†æçš„è‡ªåŠ¨ç›¸å…³æ€§åˆ¤æ–­")
        print("  â€¢ ç¡®ä¿è‡ªç„¶æŸ¥è¯¢å’Œç°å®ç›¸å…³æ€§")
        print("  â€¢ æœ‰æœºåœ°è§£å†³æ³›åŒ–é—®é¢˜")
        print()
        
        # ä»æ–‡æ¡£ç”ŸæˆæŸ¥è¯¢
        all_queries = self.query_generator.generate_document_based_queries()
        
        if len(all_queries) < 5:
            print(f"âš ï¸  åªç”Ÿæˆäº† {len(all_queries)} ä¸ªæŸ¥è¯¢ã€‚éœ€è¦è‡³å°‘5ä¸ªã€‚")
            return False
        
        # é€‰æ‹©5ä¸ªå¤šæ ·åŒ–æŸ¥è¯¢ä»¥ç¬¦åˆè¯¾ç¨‹ä½œä¸šè¦æ±‚
        self.selected_queries = self.query_generator.select_diverse_queries(all_queries, target_count=5)
        
        # åˆ›å»ºè‡ªåŠ¨ç›¸å…³æ€§åˆ¤æ–­
        self.relevance_judgments = self.query_generator.create_automatic_relevance_judgments(self.selected_queries)
        
        # è¯„ä¼°æ¯ä¸ªæŸ¥è¯¢
        print(f"\n" + "="*80)
        print("Part 4.1-4.4: è¯„ä¼°ç»“æœ")
        print("="*80)
        
        evaluation_results = []
        
        for query_info in self.selected_queries:
            result = self.evaluate_single_query(query_info)
            evaluation_results.append(result)
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        self.calculate_summary_statistics(evaluation_results)
        
        # ä¿å­˜ç»“æœ
        self.save_results(evaluation_results)
        
        print(f"\nâœ… åŸºäºæ–‡æ¡£çš„è¯„ä¼°å®Œæˆ!")
        
        return True
    
    def calculate_summary_statistics(self, results):
        """è®¡ç®—å’Œæ˜¾ç¤ºæ±‡æ€»ç»Ÿè®¡"""
        print(f"\nğŸ“ˆ æ±‡æ€»ç»Ÿè®¡")
        print("=" * 50)
        
        methods = ['LSI', 'Language_Model', 'BM25', 'Fusion']
        metrics = ['Precision@5', 'Recall@5', 'nDCG@5']
        
        print(f"{'æ–¹æ³•':<15} {'æŒ‡æ ‡':<12} {'å‡å€¼':<8} {'æ ‡å‡†å·®':<8} {'æœ€å°å€¼':<8} {'æœ€å¤§å€¼':<8}")
        print("-" * 70)
        
        for method in methods:
            for i, metric in enumerate(metrics):
                metric_key = f'{method}_{metric}'
                values = [result[metric_key] for result in results]
                
                avg_value = np.mean(values)
                std_value = np.std(values) if len(values) > 1 else 0.0
                max_value = np.max(values)
                min_value = np.min(values)
                
                method_display = method if i == 0 else ""
                print(f"{method_display:<15} {metric:<12} {avg_value:.3f}    {std_value:.3f}    {min_value:.3f}    {max_value:.3f}")
            if method != methods[-1]:
                print()
        
        # æ‰¾åˆ°æœ€ä½³æ–¹æ³•
        print(f"\nğŸ† æœ€ä½³è¡¨ç°æ–¹æ³•:")
        for metric in metrics:
            best_method = None
            best_score = -1
            
            for method in methods:
                avg_score = np.mean([result[f'{method}_{metric}'] for result in results])
                if avg_score > best_score:
                    best_score = avg_score
                    best_method = method
            
            print(f"   {metric:<12}: {best_method:<15} ({best_score:.3f})")
    
    def save_results(self, results):
        """å°†ç»“æœä¿å­˜åˆ°CSVæ–‡ä»¶"""
        print(f"\nğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ")
        print("-" * 30)
        
        os.makedirs('results', exist_ok=True)
        
        # ä¿å­˜precisionç»“æœ
        precision_data = []
        for result in results:
            row = {
                'Query_ID': result['query_id'],
                'Query_Text': result['query_text'],
                'Query_Type': result['query_type'],
                'Source_Document': result['source_document'],
                'LSI_Precision@5': result['LSI_Precision@5'],
                'Language_Model_Precision@5': result['Language_Model_Precision@5'],
                'BM25_Precision@5': result['BM25_Precision@5'],
                'Fusion_Precision@5': result['Fusion_Precision@5']
            }
            precision_data.append(row)
        
        with open('results/precision.csv', 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=precision_data[0].keys())
            writer.writeheader()
            writer.writerows(precision_data)
        
        # ä¿å­˜recallç»“æœ
        recall_data = []
        for result in results:
            row = {
                'Query_ID': result['query_id'],
                'Query_Text': result['query_text'],
                'Query_Type': result['query_type'],
                'Source_Document': result['source_document'],
                'LSI_Recall@5': result['LSI_Recall@5'],
                'Language_Model_Recall@5': result['Language_Model_Recall@5'],
                'BM25_Recall@5': result['BM25_Recall@5'],
                'Fusion_Recall@5': result['Fusion_Recall@5']
            }
            recall_data.append(row)
        
        with open('results/recall.csv', 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=recall_data[0].keys())
            writer.writeheader()
            writer.writerows(recall_data)
        
        # ä¿å­˜nDCGç»“æœ
        ndcg_data = []
        for result in results:
            row = {
                'Query_ID': result['query_id'],
                'Query_Text': result['query_text'],
                'Query_Type': result['query_type'],
                'Source_Document': result['source_document'],
                'LSI_nDCG@5': result['LSI_nDCG@5'],
                'Language_Model_nDCG@5': result['Language_Model_nDCG@5'],
                'BM25_nDCG@5': result['BM25_nDCG@5'],
                'Fusion_nDCG@5': result['Fusion_nDCG@5']
            }
            ndcg_data.append(row)
        
        with open('results/nDCG.csv', 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=ndcg_data[0].keys())
            writer.writeheader()
            writer.writerows(ndcg_data)
        
        # ä¿å­˜ç›¸å…³æ€§åˆ¤æ–­
        relevance_data = []
        for query_id, judgments in self.relevance_judgments.items():
            for doc_id, relevance in judgments.items():
                row = {
                    'Query_ID': query_id,
                    'Document_ID': doc_id,
                    'Document_Name': self.irs.document_names[doc_id] if doc_id < len(self.irs.document_names) else f'doc_{doc_id}',
                    'Relevance_Score': relevance
                }
                relevance_data.append(row)
        
        with open('results/relevance_judgments.csv', 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=relevance_data[0].keys())
            writer.writeheader()
            writer.writerows(relevance_data)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜:")
        print(f"   â€¢ precision.csv")
        print(f"   â€¢ recall.csv") 
        print(f"   â€¢ nDCG.csv")
        print(f"   â€¢ relevance_judgments.csv")


def main():
    """è¿è¡ŒåŸºäºæ–‡æ¡£çš„è¯„ä¼°"""
    evaluator = DocumentBasedEvaluator()
    
    if evaluator.run_document_based_evaluation():
        print(f"\nğŸ‰ åŸºäºæ–‡æ¡£çš„è¯„ä¼°å®Œæˆ!")
        print(f"ğŸ“Š ä»å®é™…æ–‡æ¡£å†…å®¹ç”Ÿæˆäº†è‡ªç„¶æŸ¥è¯¢")
        print(f"ğŸ“„ è‡ªåŠ¨ç›¸å…³æ€§åˆ¤æ–­ç¡®ä¿äº†ç°å®è¯„ä¼°")
        print(f"ğŸ“ è¯¾ç¨‹ä½œä¸šæ–‡ä»¶å·²åœ¨'results/'ç›®å½•ä¸­å‡†å¤‡å°±ç»ª")
    else:
        print(f"\nâŒ è¯„ä¼°å¤±è´¥!")


if __name__ == "__main__":
    main()