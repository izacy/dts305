import os
import re
import csv
import math
import random
import numpy as np
from datetime import datetime
from collections import defaultdict, Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# å°è¯•å¯¼å…¥NLTKï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk.util import ngrams
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    print("è­¦å‘Š: NLTKä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–çš„æ–‡æœ¬å¤„ç†æ–¹æ³•")

from main_system import InformationRetrievalSystem


class ImprovedQueryGenerator:
    """
    æ”¹è¿›çš„æŸ¥è¯¢ç”Ÿæˆå™¨ - æ¶ˆé™¤äººä¸ºåå‘ï¼Œæä¾›ç§‘å­¦ä¸¥è°¨çš„è¯„ä¼°
    """
    
    def __init__(self):
        self.irs = InformationRetrievalSystem()
        
        # åœç”¨è¯è®¾ç½®
        if NLTK_AVAILABLE:
            try:
                self.stop_words = set(stopwords.words('english'))
            except:
                nltk.download('punkt')
                nltk.download('stopwords')
                self.stop_words = set(stopwords.words('english'))
        else:
            # å¤‡ç”¨åœç”¨è¯åˆ—è¡¨
            self.stop_words = {
                'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',
                'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',
                'to', 'was', 'will', 'with', 'the', 'this', 'but', 'they', 'have',
                'had', 'what', 'said', 'each', 'which', 'she', 'do', 'how', 'their',
                'if', 'up', 'out', 'many', 'then', 'them', 'these', 'so', 'some',
                'her', 'would', 'make', 'like', 'into', 'him', 'time', 'two', 'more',
                'go', 'no', 'way', 'could', 'my', 'than', 'first', 'been', 'call',
                'who', 'oil', 'sit', 'now', 'find', 'down', 'day', 'did', 'get',
                'come', 'made', 'may', 'part'
            }
        
        # æ‰©å±•åœç”¨è¯
        self.stop_words.update(['said', 'also', 'would', 'could', 'one', 'two', 
                               'first', 'last', 'new', 'old', 'good', 'well',
                               'way', 'much', 'many', 'take', 'make', 'get'])
    
    def simple_tokenize(self, text):
        """ç®€å•åˆ†è¯æ–¹æ³•ï¼ˆNLTKä¸å¯ç”¨æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        if NLTK_AVAILABLE:
            return word_tokenize(text.lower())
        else:
            # ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼åˆ†è¯
            words = re.findall(r'\b[a-zA-Z]{3,}\b', text.lower())
            return words
    
    def generate_document_based_queries(self):
        """
        ç”Ÿæˆæ”¹è¿›çš„è‡ªåŠ¨æŸ¥è¯¢é›†åˆ
        
        Returns:
            list: å¤šæ ·åŒ–æŸ¥è¯¢åˆ—è¡¨
        """
        print("ğŸ” ç”Ÿæˆæ”¹è¿›çš„è‡ªåŠ¨æŸ¥è¯¢é›†åˆ")
        print("=" * 60)
        
        # åˆå§‹åŒ–IRç³»ç»Ÿ
        if not self.irs.initialize_system():
            print("Failed to load documents")
            return []
        
        all_queries = []
        
        # 1. åŸºäºTF-IDFçš„é‡è¦è¯æ±‡æŸ¥è¯¢
        tfidf_queries = self._generate_tfidf_queries()
        all_queries.extend(tfidf_queries)
        
        # 2. åŸºäºè¯æ±‡å…±ç°çš„æŸ¥è¯¢
        cooccurrence_queries = self._generate_cooccurrence_queries()
        all_queries.extend(cooccurrence_queries)
        
        # 3. è·¨æ–‡æ¡£ç›¸ä¼¼æ€§æŸ¥è¯¢
        similarity_queries = self._generate_cross_document_queries()
        all_queries.extend(similarity_queries)
        
        # 4. è´Ÿé¢æŸ¥è¯¢ï¼ˆæµ‹è¯•é²æ£’æ€§ï¼‰
        negative_queries = self._generate_negative_queries()
        all_queries.extend(negative_queries)
        
        # 5. é•¿åº¦åˆ†å±‚æŸ¥è¯¢
        length_queries = self._generate_length_varied_queries()
        all_queries.extend(length_queries)
        
        # é€‰æ‹©æœ€ç»ˆçš„å¤šæ ·åŒ–æŸ¥è¯¢é›†åˆ
        selected_queries = self._select_diverse_queries(all_queries, target_count=5)
        
        print(f"\nâœ… ç”Ÿæˆäº† {len(selected_queries)} ä¸ªæ”¹è¿›æŸ¥è¯¢")
        return selected_queries
    
    def _generate_tfidf_queries(self):
        """åŸºäºTF-IDFæƒé‡ç”Ÿæˆå®¢è§‚é‡è¦è¯æ±‡æŸ¥è¯¢"""
        print("ğŸ“Š 1. åŸºäºTF-IDFç”Ÿæˆé‡è¦è¯æ±‡æŸ¥è¯¢...")
        
        # é¢„å¤„ç†æ–‡æ¡£
        processed_docs = []
        for doc in self.irs.documents:
            processed = self.irs.preprocessor.preprocess_text(doc)
            processed_docs.append(processed)
        
        # è®¡ç®—TF-IDF
        try:
            vectorizer = TfidfVectorizer(
                max_features=500,
                min_df=1,
                max_df=0.8,
                ngram_range=(1, 1),
                stop_words='english' if not self.stop_words else list(self.stop_words)
            )
            tfidf_matrix = vectorizer.fit_transform(processed_docs)
            feature_names = vectorizer.get_feature_names_out()
        except Exception as e:
            print(f"   TF-IDFè®¡ç®—å¤±è´¥: {e}")
            return []
        
        queries = []
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£ç”ŸæˆåŸºäºé‡è¦è¯æ±‡çš„æŸ¥è¯¢
        for doc_idx, doc_name in enumerate(self.irs.document_names):
            if doc_idx < tfidf_matrix.shape[0]:
                doc_scores = tfidf_matrix[doc_idx].toarray()[0]
                
                # è·å–å‰5ä¸ªé‡è¦è¯æ±‡
                top_indices = np.argsort(doc_scores)[::-1][:5]
                top_words = []
                for idx in top_indices:
                    if doc_scores[idx] > 0:
                        word = feature_names[idx]
                        if len(word) > 2 and word not in self.stop_words:
                            top_words.append(word)
                
                if len(top_words) >= 2:
                    # å•è¯æŸ¥è¯¢
                    queries.append({
                        'query': top_words[0],
                        'type': 'tfidf_single',
                        'source_document': doc_name,
                        'source_doc_index': doc_idx,
                        'expected_category': doc_name.split('_')[0] if '_' in doc_name else 'main',
                        'tfidf_score': float(doc_scores[top_indices[0]])
                    })
                    
                    # åŒè¯ç»„åˆæŸ¥è¯¢
                    if len(top_words) >= 2:
                        queries.append({
                            'query': f"{top_words[0]} {top_words[1]}",
                            'type': 'tfidf_dual',
                            'source_document': doc_name,
                            'source_doc_index': doc_idx,
                            'expected_category': doc_name.split('_')[0] if '_' in doc_name else 'main',
                            'tfidf_score': float((doc_scores[top_indices[0]] + doc_scores[top_indices[1]]) / 2)
                        })
        
        print(f"   âœ“ ç”Ÿæˆ {len(queries)} ä¸ªTF-IDFæŸ¥è¯¢")
        return queries
    
    def _generate_cooccurrence_queries(self):
        """åŸºäºè¯æ±‡å…±ç°æ¨¡å¼ç”ŸæˆæŸ¥è¯¢"""
        print("ğŸ”— 2. åŸºäºè¯æ±‡å…±ç°ç”ŸæˆæŸ¥è¯¢...")
        
        # æ„å»ºå…±ç°ç»Ÿè®¡
        cooccurrence = defaultdict(lambda: defaultdict(int))
        window_size = 5
        
        for doc in self.irs.documents:
            words = self.simple_tokenize(doc)
            # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
            words = [w for w in words if len(w) > 2 and w not in self.stop_words]
            
            # ç»Ÿè®¡çª—å£å†…å…±ç°
            for i, word1 in enumerate(words):
                start = max(0, i - window_size)
                end = min(len(words), i + window_size + 1)
                for j in range(start, end):
                    if i != j:
                        word2 = words[j]
                        cooccurrence[word1][word2] += 1
        
        queries = []
        
        # ç”Ÿæˆå…±ç°æŸ¥è¯¢
        for word1, cooccur_dict in cooccurrence.items():
            if len(cooccur_dict) >= 2:
                # æŒ‰å…±ç°é¢‘ç‡æ’åº
                sorted_pairs = sorted(cooccur_dict.items(), key=lambda x: x[1], reverse=True)
                
                for word2, freq in sorted_pairs[:1]:  # åªå–æœ€å¼ºå…±ç°
                    if freq >= 2:  # è‡³å°‘å…±ç°2æ¬¡
                        queries.append({
                            'query': f"{word1} {word2}",
                            'type': 'cooccurrence',
                            'source_document': 'multiple',
                            'source_doc_index': -1,
                            'expected_category': 'cross_category',
                            'cooccurrence_freq': freq
                        })
        
        # æŒ‰å…±ç°å¼ºåº¦æ’åºï¼Œé€‰æ‹©å‰å‡ ä¸ª
        queries.sort(key=lambda x: x['cooccurrence_freq'], reverse=True)
        result = queries[:3]  # é™åˆ¶æ•°é‡
        
        print(f"   âœ“ ç”Ÿæˆ {len(result)} ä¸ªå…±ç°æŸ¥è¯¢")
        return result
    
    def _generate_cross_document_queries(self):
        """åŸºäºæ–‡æ¡£é—´ç›¸ä¼¼æ€§ç”Ÿæˆè·¨æ–‡æ¡£æŸ¥è¯¢"""
        print("ğŸ”„ 3. ç”Ÿæˆè·¨æ–‡æ¡£ç›¸ä¼¼æ€§æŸ¥è¯¢...")
        
        # è®¡ç®—æ–‡æ¡£ç›¸ä¼¼æ€§
        processed_docs = [self.irs.preprocessor.preprocess_text(doc) for doc in self.irs.documents]
        
        try:
            vectorizer = TfidfVectorizer(max_features=200, min_df=1, max_df=0.9)
            tfidf_matrix = vectorizer.fit_transform(processed_docs)
            similarity_matrix = cosine_similarity(tfidf_matrix)
            feature_names = vectorizer.get_feature_names_out()
        except Exception as e:
            print(f"   è·¨æ–‡æ¡£æŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {e}")
            return []
        
        queries = []
        
        # æ‰¾åˆ°ä¸­ç­‰ç›¸ä¼¼åº¦çš„æ–‡æ¡£å¯¹
        for i in range(len(self.irs.documents)):
            for j in range(i + 1, len(self.irs.documents)):
                similarity = similarity_matrix[i][j]
                
                if 0.1 < similarity < 0.7:  # ä¸­ç­‰ç›¸ä¼¼åº¦
                    # æ‰¾åˆ°å…±åŒé‡è¦è¯æ±‡
                    doc1_scores = tfidf_matrix[i].toarray()[0]
                    doc2_scores = tfidf_matrix[j].toarray()[0]
                    
                    common_words = []
                    for idx, (score1, score2) in enumerate(zip(doc1_scores, doc2_scores)):
                        if score1 > 0.1 and score2 > 0.1:
                            word = feature_names[idx]
                            if len(word) > 2 and word not in self.stop_words:
                                common_words.append((word, min(score1, score2)))
                    
                    if len(common_words) >= 2:
                        # æŒ‰é‡è¦æ€§æ’åº
                        common_words.sort(key=lambda x: x[1], reverse=True)
                        top_words = [word for word, score in common_words[:2]]
                        
                        queries.append({
                            'query': ' '.join(top_words),
                            'type': 'cross_document',
                            'source_document': f"{self.irs.document_names[i]}+{self.irs.document_names[j]}",
                            'source_doc_index': [i, j],
                            'expected_category': 'cross_category',
                            'similarity': float(similarity)
                        })
        
        # é€‰æ‹©å‰å‡ ä¸ª
        queries.sort(key=lambda x: x['similarity'], reverse=True)
        result = queries[:2]
        
        print(f"   âœ“ ç”Ÿæˆ {len(result)} ä¸ªè·¨æ–‡æ¡£æŸ¥è¯¢")
        return result
    
    def _generate_negative_queries(self):
        """ç”Ÿæˆè´Ÿé¢æŸ¥è¯¢æµ‹è¯•ç³»ç»Ÿé²æ£’æ€§"""
        print("âŒ 4. ç”Ÿæˆè´Ÿé¢æŸ¥è¯¢...")
        
        # æ”¶é›†æ–‡æ¡£ä¸­çš„æ‰€æœ‰è¯æ±‡
        all_words = set()
        for doc in self.irs.documents:
            words = self.simple_tokenize(doc)
            all_words.update(words)
        
        # é¢„å®šä¹‰çš„æ— å…³é¢†åŸŸè¯æ±‡
        irrelevant_domains = [
            ['cooking', 'recipe', 'ingredient', 'kitchen', 'chef'],
            ['weather', 'temperature', 'rain', 'sunny', 'cloud'],
            ['mathematics', 'equation', 'formula', 'calculate', 'algebra'],
            ['music', 'song', 'melody', 'instrument', 'concert'],
            ['travel', 'journey', 'destination', 'vacation', 'tourist']
        ]
        
        queries = []
        
        for domain_words in irrelevant_domains:
            # æ£€æŸ¥è¿™äº›è¯æ˜¯å¦çœŸçš„ä¸åœ¨æ–‡æ¡£ä¸­
            words_in_docs = [word for word in domain_words if word in all_words]
            
            if len(words_in_docs) == 0:  # ç¡®ä¿å®Œå…¨æ— å…³
                # å•è¯æŸ¥è¯¢
                queries.append({
                    'query': domain_words[0],
                    'type': 'negative_single',
                    'source_document': 'none',
                    'source_doc_index': -1,
                    'expected_category': 'irrelevant'
                })
                break  # åªéœ€è¦ä¸€ä¸ªè´Ÿé¢æŸ¥è¯¢
        
        print(f"   âœ“ ç”Ÿæˆ {len(queries)} ä¸ªè´Ÿé¢æŸ¥è¯¢")
        return queries
    
    def _generate_length_varied_queries(self):
        """ç”Ÿæˆä¸åŒé•¿åº¦çš„æŸ¥è¯¢æµ‹è¯•ç®—æ³•å¯¹å¤æ‚åº¦çš„å¤„ç†"""
        print("ğŸ“ 5. ç”Ÿæˆé•¿åº¦åˆ†å±‚æŸ¥è¯¢...")
        
        queries = []
        
        # çŸ­æŸ¥è¯¢ (1è¯) - åŸºäºé«˜é¢‘é‡è¦è¯
        processed_docs = [self.irs.preprocessor.preprocess_text(doc) for doc in self.irs.documents]
        all_words = []
        for doc in processed_docs:
            all_words.extend(doc.split())
        
        word_freq = Counter(all_words)
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        important_words = [word for word, freq in word_freq.most_common(20) 
                          if len(word) > 3 and word not in self.stop_words and freq >= 2]
        
        if important_words:
            queries.append({
                'query': important_words[0],
                'type': 'short_query',
                'source_document': 'multiple',
                'source_doc_index': -1,
                'expected_category': 'general',
                'length': 1
            })
        
        print(f"   âœ“ ç”Ÿæˆ {len(queries)} ä¸ªé•¿åº¦åˆ†å±‚æŸ¥è¯¢")
        return queries
    
    def _select_diverse_queries(self, all_queries, target_count=5):
        """é€‰æ‹©å¤šæ ·åŒ–çš„æŸ¥è¯¢å­é›†"""
        print(f"\nğŸ¯ ä» {len(all_queries)} ä¸ªå€™é€‰æŸ¥è¯¢ä¸­é€‰æ‹© {target_count} ä¸ª...")
        
        if len(all_queries) <= target_count:
            # å¦‚æœæŸ¥è¯¢æ•°é‡ä¸è¶³ï¼Œç›´æ¥è¿”å›æ‰€æœ‰æŸ¥è¯¢
            for i, query in enumerate(all_queries, 1):
                query['id'] = f'Q{i}'
            return all_queries
        
        # æŒ‰ç±»å‹åˆ†ç»„
        by_type = defaultdict(list)
        for query in all_queries:
            by_type[query['type']].append(query)
        
        selected = []
        
        # ç¡®ä¿æ¯ç§ç±»å‹è‡³å°‘æœ‰ä¸€ä¸ªä»£è¡¨
        type_list = list(by_type.keys())
        queries_per_type = target_count // len(type_list)
        remainder = target_count % len(type_list)
        
        for i, query_type in enumerate(type_list):
            count = queries_per_type + (1 if i < remainder else 0)
            type_queries = by_type[query_type]
            
            # æ ¹æ®ç±»å‹é€‰æ‹©æœ€ä½³æŸ¥è¯¢
            if query_type in ['tfidf_single', 'tfidf_dual']:
                # TF-IDFæŸ¥è¯¢æŒ‰åˆ†æ•°æ’åº
                type_queries.sort(key=lambda x: x.get('tfidf_score', 0), reverse=True)
            elif query_type == 'cooccurrence':
                # å…±ç°æŸ¥è¯¢æŒ‰é¢‘ç‡æ’åº
                type_queries.sort(key=lambda x: x.get('cooccurrence_freq', 0), reverse=True)
            elif query_type == 'cross_document':
                # è·¨æ–‡æ¡£æŸ¥è¯¢æŒ‰ç›¸ä¼¼åº¦æ’åº
                type_queries.sort(key=lambda x: x.get('similarity', 0), reverse=True)
            else:
                # å…¶ä»–ç±»å‹éšæœºé€‰æ‹©
                random.shuffle(type_queries)
            
            # é€‰æ‹©è¯¥ç±»å‹çš„æŸ¥è¯¢
            selected.extend(type_queries[:count])
        
        # å¦‚æœä»ç„¶ä¸å¤Ÿï¼Œä»å‰©ä½™æŸ¥è¯¢ä¸­éšæœºé€‰æ‹©
        while len(selected) < target_count:
            remaining = [q for q in all_queries if q not in selected]
            if not remaining:
                break
            selected.append(random.choice(remaining))
        
        # æˆªå–åˆ°ç›®æ ‡æ•°é‡å¹¶é‡æ–°ç¼–å·
        selected = selected[:target_count]
        for i, query in enumerate(selected, 1):
            query['id'] = f'Q{i}'
        
        # æ˜¾ç¤ºé€‰æ‹©çš„æŸ¥è¯¢åˆ†å¸ƒ
        print("\nğŸ“‹ é€‰æ‹©çš„æŸ¥è¯¢ç±»å‹åˆ†å¸ƒ:")
        type_counts = Counter(q['type'] for q in selected)
        for qtype, count in type_counts.items():
            print(f"   â€¢ {qtype}: {count} ä¸ª")
        
        return selected
    
    def create_automatic_relevance_judgments(self, queries):
        """
        ä¸ºæŸ¥è¯¢åˆ›å»ºæ”¹è¿›çš„è‡ªåŠ¨ç›¸å…³æ€§åˆ¤æ–­
        
        Args:
            queries (list): æŸ¥è¯¢åˆ—è¡¨
            
        Returns:
            dict: ç›¸å…³æ€§åˆ¤æ–­å­—å…¸
        """
        print(f"\nğŸ¯ åˆ›å»ºæ”¹è¿›çš„è‡ªåŠ¨ç›¸å…³æ€§åˆ¤æ–­")
        print("-" * 50)
        
        relevance_judgments = {}
        
        # é¢„å¤„ç†æ–‡æ¡£ç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
        processed_docs = [self.irs.preprocessor.preprocess_text(doc) for doc in self.irs.documents]
        
        # æ„å»ºTF-IDFå‘é‡ç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦
        try:
            vectorizer = TfidfVectorizer(max_features=300, min_df=1, max_df=0.9)
            doc_vectors = vectorizer.fit_transform(processed_docs)
        except Exception as e:
            print(f"   è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ç®€å•åŒ¹é…: {e}")
            doc_vectors = None
            vectorizer = None
        
        for query_info in queries:
            query_id = query_info['id']
            query_text = query_info['query']
            query_type = query_info['type']
            
            relevance = {}
            
            # è®¡ç®—æŸ¥è¯¢å‘é‡ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            query_vector = None
            if vectorizer is not None:
                try:
                    query_processed = self.irs.preprocessor.preprocess_text(query_text)
                    query_vector = vectorizer.transform([query_processed])
                except:
                    pass
            
            for doc_idx in range(len(self.irs.documents)):
                doc_content = processed_docs[doc_idx]
                query_words = query_text.lower().split()
                
                # 1. åŸºç¡€è¯æ±‡åŒ¹é…åº¦
                word_matches = sum(1 for word in query_words if word in doc_content)
                match_ratio = word_matches / len(query_words) if query_words else 0
                
                # 2. è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                semantic_sim = 0.0
                if query_vector is not None and doc_vectors is not None:
                    try:
                        sim_matrix = cosine_similarity(query_vector, doc_vectors[doc_idx:doc_idx+1])
                        semantic_sim = sim_matrix[0][0]
                    except:
                        pass
                
                # 3. ç»¼åˆç›¸å…³æ€§è®¡ç®—
                relevance_score = self._calculate_relevance_score(
                    query_info, doc_idx, match_ratio, semantic_sim
                )
                
                relevance[doc_idx] = relevance_score
            
            relevance_judgments[query_id] = relevance
            
            # æ˜¾ç¤ºç›¸å…³æ€§åˆ¤æ–­æ‘˜è¦
            relevant_docs = sum(1 for score in relevance.values() if score > 0)
            highly_relevant = sum(1 for score in relevance.values() if score >= 2)
            
            print(f"   {query_id}: '{query_text}' - {relevant_docs} ç›¸å…³æ–‡æ¡£ ({highly_relevant} é«˜åº¦ç›¸å…³)")
        
        return relevance_judgments
    
    def _calculate_relevance_score(self, query_info, doc_idx, match_ratio, semantic_sim):
        """
        è®¡ç®—æ”¹è¿›çš„ç›¸å…³æ€§åˆ†æ•°
        
        Args:
            query_info: æŸ¥è¯¢ä¿¡æ¯
            doc_idx: æ–‡æ¡£ç´¢å¼•
            match_ratio: è¯æ±‡åŒ¹é…æ¯”ä¾‹
            semantic_sim: è¯­ä¹‰ç›¸ä¼¼åº¦
            
        Returns:
            int: ç›¸å…³æ€§åˆ†æ•° (0-3)
        """
        query_type = query_info['type']
        
        # è´Ÿé¢æŸ¥è¯¢åº”è¯¥ä¸ç›¸å…³
        if query_type in ['negative_single', 'negative_dual']:
            return 0
        
        # ç»„åˆåŒ¹é…åº¦å’Œè¯­ä¹‰ç›¸ä¼¼åº¦
        combined_score = 0.7 * match_ratio + 0.3 * semantic_sim
        
        # æ ¹æ®æŸ¥è¯¢ç±»å‹è°ƒæ•´ç›¸å…³æ€§åˆ¤æ–­
        if query_type in ['tfidf_single', 'tfidf_dual']:
            # TF-IDFæŸ¥è¯¢ï¼šå¯¹æºæ–‡æ¡£ç»™äºˆé€‚åº¦åé‡
            if isinstance(query_info['source_doc_index'], int) and doc_idx == query_info['source_doc_index']:
                # æºæ–‡æ¡£ï¼Œä½†ä¸è‡ªåŠ¨ç»™æœ€é«˜åˆ†ï¼ŒåŸºäºå®é™…åŒ¹é…åº¦
                if combined_score >= 0.6:
                    return 3
                elif combined_score >= 0.4:
                    return 2
                else:
                    return 1
            else:
                # éæºæ–‡æ¡£ï¼Œçº¯ç²¹åŸºäºåŒ¹é…åº¦
                if combined_score >= 0.8:
                    return 2
                elif combined_score >= 0.5:
                    return 1
                else:
                    return 0
        
        elif query_type == 'cross_document':
            # è·¨æ–‡æ¡£æŸ¥è¯¢ï¼šå¯¹ç›®æ ‡æ–‡æ¡£å…¬å¹³å¯¹å¾…
            if isinstance(query_info['source_doc_index'], list) and doc_idx in query_info['source_doc_index']:
                if combined_score >= 0.5:
                    return 2
                else:
                    return 1
            else:
                if combined_score >= 0.7:
                    return 1
                else:
                    return 0
        
        elif query_type == 'cooccurrence':
            # å…±ç°æŸ¥è¯¢ï¼šåŸºäºå®é™…åŒ¹é…æƒ…å†µ
            if combined_score >= 0.7:
                return 2
            elif combined_score >= 0.4:
                return 1
            else:
                return 0
        
        else:
            # å…¶ä»–æŸ¥è¯¢ç±»å‹ï¼šæ ‡å‡†åˆ¤æ–­
            if combined_score >= 0.8:
                return 2
            elif combined_score >= 0.5:
                return 1
            else:
                return 0


class DocumentBasedEvaluator:
    """
    æ”¹è¿›çš„æ–‡æ¡£è¯„ä¼°å™¨
    """
    
    def __init__(self):
        self.query_generator = ImprovedQueryGenerator()
        self.irs = self.query_generator.irs
        self.selected_queries = []
        self.relevance_judgments = {}
        
    def calculate_precision_at_k(self, retrieved_docs, relevant_docs, k=5):
        """è®¡ç®—Precision@K"""
        if not retrieved_docs or k == 0:
            return 0.0
        
        top_k_docs = [doc_id for doc_id, _ in retrieved_docs[:k]]
        relevant_retrieved = sum(1 for doc_id in top_k_docs 
                               if relevant_docs.get(doc_id, 0) > 0)
        
        return relevant_retrieved / k
    
    def calculate_recall_at_k(self, retrieved_docs, relevant_docs, k=5):
        """è®¡ç®—Recall@K"""
        if not retrieved_docs or k == 0:
            return 0.0
        
        total_relevant = sum(1 for rel in relevant_docs.values() if rel > 0)
        if total_relevant == 0:
            return 0.0
        
        top_k_docs = [doc_id for doc_id, _ in retrieved_docs[:k]]
        relevant_retrieved = sum(1 for doc_id in top_k_docs 
                               if relevant_docs.get(doc_id, 0) > 0)
        
        return relevant_retrieved / total_relevant
    
    def calculate_dcg_at_k(self, retrieved_docs, relevant_docs, k=5):
        """è®¡ç®—DCG@K"""
        if not retrieved_docs or k == 0:
            return 0.0
        
        dcg = 0.0
        for i, (doc_id, _) in enumerate(retrieved_docs[:k]):
            relevance = relevant_docs.get(doc_id, 0)
            
            if i == 0:
                dcg += relevance
            else:
                dcg += relevance / math.log2(i + 2)
        
        return dcg
    
    def calculate_ideal_dcg_at_k(self, relevant_docs, k=5):
        """è®¡ç®—Ideal DCG@K"""
        sorted_relevances = sorted(relevant_docs.values(), reverse=True)
        
        idcg = 0.0
        for i, relevance in enumerate(sorted_relevances[:k]):
            if i == 0:
                idcg += relevance
            else:
                idcg += relevance / math.log2(i + 2)
        
        return idcg
    
    def calculate_ndcg_at_k(self, retrieved_docs, relevant_docs, k=5):
        """è®¡ç®—nDCG@K"""
        dcg = self.calculate_dcg_at_k(retrieved_docs, relevant_docs, k)
        idcg = self.calculate_ideal_dcg_at_k(relevant_docs, k)
        
        if idcg == 0:
            return 0.0
        
        return dcg / idcg
    
    def calculate_fusion_results(self, lsi_results, lm_results, alpha=0.6):
        """è®¡ç®—èåˆç»“æœ"""
        def normalize_scores(results):
            if not results:
                return {}
            scores = [score for _, score in results]
            min_score, max_score = min(scores), max(scores)
            if max_score == min_score:
                return {doc_idx: 0.5 for doc_idx, _ in results}
            return {doc_idx: (score - min_score) / (max_score - min_score) 
                   for doc_idx, score in results}
        
        lsi_scores = normalize_scores(lsi_results)
        lm_scores = normalize_scores(lm_results)
        all_doc_indices = set(lsi_scores.keys()) | set(lm_scores.keys())
        
        fusion_results = []
        for doc_idx in all_doc_indices:
            lsi_score = lsi_scores.get(doc_idx, 0)
            lm_score = lm_scores.get(doc_idx, 0)
            fusion_score = alpha * lsi_score + (1 - alpha) * lm_score
            fusion_results.append((doc_idx, fusion_score))
        
        fusion_results.sort(key=lambda x: x[1], reverse=True)
        return fusion_results
    
    def evaluate_single_query(self, query_info):
        """è¯„ä¼°å•ä¸ªæŸ¥è¯¢"""
        query_id = query_info['id']
        query_text = query_info['query']
        
        print(f"\nğŸ” è¯„ä¼° {query_id}: '{query_text}' (ç±»å‹: {query_info['type']})")
        
        # è·å–ç»“æœ
        lsi_results = self.irs.information_retrieval_algorithm_1(query_text, top_k=5)
        lm_results = self.irs.information_retrieval_algorithm_2(query_text, top_k=5)
        fusion_results = self.calculate_fusion_results(lsi_results, lm_results)
        
        # è·å–ç›¸å…³æ€§åˆ¤æ–­
        relevant_docs = self.relevance_judgments[query_id]
        
        # è®¡ç®—æŒ‡æ ‡
        result = {
            'query_id': query_id,
            'query_text': query_text,
            'query_type': query_info['type'],
            'source_document': query_info.get('source_document', 'unknown'),
            'expected_category': query_info.get('expected_category', 'unknown')
        }
        
        methods = [
            ('LSI', lsi_results),
            ('Language_Model', lm_results),
            ('Fusion', fusion_results)
        ]
        
        for method_name, retrieved_docs in methods:
            precision = self.calculate_precision_at_k(retrieved_docs, relevant_docs, k=5)
            recall = self.calculate_recall_at_k(retrieved_docs, relevant_docs, k=5)
            ndcg = self.calculate_ndcg_at_k(retrieved_docs, relevant_docs, k=5)
            
            result[f'{method_name}_Precision@5'] = precision
            result[f'{method_name}_Recall@5'] = recall
            result[f'{method_name}_nDCG@5'] = ndcg
            
            print(f"   {method_name:12}: P@5={precision:.3f}, R@5={recall:.3f}, nDCG@5={ndcg:.3f}")
            
            # æ˜¾ç¤ºå‰3ä¸ªæ£€ç´¢ç»“æœ
            if retrieved_docs:
                top_3 = [(self.irs.document_names[doc_id], score, relevant_docs.get(doc_id, 0)) 
                        for doc_id, score in retrieved_docs[:3]]
                print(f"      å‰3å: {[(name.split('.')[0], f'{score:.3f}', f'rel={rel}') for name, score, rel in top_3]}")
        
        return result
    
    def run_document_based_evaluation(self):
        """è¿è¡Œå®Œæ•´çš„æ”¹è¿›è¯„ä¼°"""
        print("ğŸš€ æ”¹è¿›çš„æ–‡æ¡£åŸºç¡€æŸ¥è¯¢è¯„ä¼°")
        print("DTS305TC Natural Language Processing - æ”¹è¿›ç‰ˆ Part 4")
        print("=" * 80)
        
        print("ğŸ¯ æ”¹è¿›æ–¹æ¡ˆç‰¹ç‚¹:")
        print("  â€¢ åŸºäºTF-IDFçš„å®¢è§‚é‡è¦æ€§")
        print("  â€¢ è¯æ±‡å…±ç°æ¨¡å¼åˆ†æ")
        print("  â€¢ è·¨æ–‡æ¡£ç›¸ä¼¼æ€§æµ‹è¯•")
        print("  â€¢ è´Ÿé¢æŸ¥è¯¢é²æ£’æ€§æµ‹è¯•")
        print("  â€¢ è¯­ä¹‰ç›¸ä¼¼åº¦å¢å¼ºç›¸å…³æ€§åˆ¤æ–­")
        print("  â€¢ æ¶ˆé™¤äººä¸ºåå‘ï¼Œæä¾›ç§‘å­¦è¯„ä¼°")
        print()
        
        # ç”Ÿæˆæ”¹è¿›çš„æŸ¥è¯¢
        all_queries = self.query_generator.generate_document_based_queries()
        
        if len(all_queries) < 5:
            print(f"âš ï¸  åªç”Ÿæˆäº† {len(all_queries)} ä¸ªæŸ¥è¯¢ï¼Œç»§ç»­è¯„ä¼°...")
        
        self.selected_queries = all_queries
        
        # åˆ›å»ºæ”¹è¿›çš„ç›¸å…³æ€§åˆ¤æ–­
        self.relevance_judgments = self.query_generator.create_automatic_relevance_judgments(self.selected_queries)
        
        # è¯„ä¼°æ¯ä¸ªæŸ¥è¯¢
        print(f"\n" + "="*80)
        print("Part 4.1-4.4: æ”¹è¿›è¯„ä¼°ç»“æœ")
        print("="*80)
        
        evaluation_results = []
        
        for query_info in self.selected_queries:
            result = self.evaluate_single_query(query_info)
            evaluation_results.append(result)
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        self.calculate_summary_statistics(evaluation_results)
        
        # ä¿å­˜ç»“æœ
        self.save_results(evaluation_results)
        
        print(f"\nâœ… æ”¹è¿›çš„æ–‡æ¡£åŸºç¡€è¯„ä¼°å®Œæˆ!")
        print(f"ğŸ“Š ä½¿ç”¨ç§‘å­¦æ–¹æ³•ç”Ÿæˆå¤šæ ·åŒ–æŸ¥è¯¢")
        print(f"ğŸ¯ æ¶ˆé™¤äººä¸ºåå‘ï¼Œæä¾›å®¢è§‚è¯„ä¼°")
        print(f"ğŸ“„ è¯¾ç¨‹ä½œä¸šæ–‡ä»¶å·²ä¿å­˜åœ¨ 'results/' ç›®å½•")
        
        return True
    
    def calculate_summary_statistics(self, results):
        """è®¡ç®—å’Œæ˜¾ç¤ºæ±‡æ€»ç»Ÿè®¡"""
        print(f"\nğŸ“ˆ æ±‡æ€»ç»Ÿè®¡")
        print("=" * 50)
        
        methods = ['LSI', 'Language_Model', 'Fusion']
        metrics = ['Precision@5', 'Recall@5', 'nDCG@5']
        
        print(f"{'æ–¹æ³•':<15} {'æŒ‡æ ‡':<12} {'å‡å€¼':<8} {'æ ‡å‡†å·®':<8} {'æœ€å°å€¼':<8} {'æœ€å¤§å€¼':<8}")
        print("-" * 70)
        
        for method in methods:
            for i, metric in enumerate(metrics):
                metric_key = f'{method}_{metric}'
                values = [result[metric_key] for result in results]
                
                avg_value = np.mean(values)
                std_value = np.std(values) if len(values) > 1 else 0.0
                max_value = np.max(values)
                min_value = np.min(values)
                
                method_display = method if i == 0 else ""
                print(f"{method_display:<15} {metric:<12} {avg_value:.3f}    {std_value:.3f}    {min_value:.3f}    {max_value:.3f}")
            if method != methods[-1]:
                print()
        
        # æ‰¾åˆ°æœ€ä½³æ–¹æ³•
        print(f"\nğŸ† æœ€ä½³è¡¨ç°æ–¹æ³•:")
        for metric in metrics:
            best_method = None
            best_score = -1
            
            for method in methods:
                avg_score = np.mean([result[f'{method}_{metric}'] for result in results])
                if avg_score > best_score:
                    best_score = avg_score
                    best_method = method
            
            print(f"   {metric:<12}: {best_method:<15} ({best_score:.3f})")
    
    def save_results(self, results):
        """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
        print(f"\nğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ")
        print("-" * 30)
        
        os.makedirs('results', exist_ok=True)
        
        # ä¿å­˜precisionç»“æœ
        precision_data = []
        for result in results:
            row = {
                'Query_ID': result['query_id'],
                'Query_Text': result['query_text'],
                'Query_Type': result['query_type'],
                'Source_Document': result['source_document'],
                'LSI_Precision@5': result['LSI_Precision@5'],
                'Language_Model_Precision@5': result['Language_Model_Precision@5'],
                'Fusion_Precision@5': result['Fusion_Precision@5']
            }
            precision_data.append(row)
        
        with open('results/precision.csv', 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=precision_data[0].keys())
            writer.writeheader()
            writer.writerows(precision_data)
        
        # ä¿å­˜recallç»“æœ
        recall_data = []
        for result in results:
            row = {
                'Query_ID': result['query_id'],
                'Query_Text': result['query_text'],
                'Query_Type': result['query_type'],
                'Source_Document': result['source_document'],
                'LSI_Recall@5': result['LSI_Recall@5'],
                'Language_Model_Recall@5': result['Language_Model_Recall@5'],
                'Fusion_Recall@5': result['Fusion_Recall@5']
            }
            recall_data.append(row)
        
        with open('results/recall.csv', 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=recall_data[0].keys())
            writer.writeheader()
            writer.writerows(recall_data)
        
        # ä¿å­˜nDCGç»“æœ
        ndcg_data = []
        for result in results:
            row = {
                'Query_ID': result['query_id'],
                'Query_Text': result['query_text'],
                'Query_Type': result['query_type'],
                'Source_Document': result['source_document'],
                'LSI_nDCG@5': result['LSI_nDCG@5'],
                'Language_Model_nDCG@5': result['Language_Model_nDCG@5'],
                'Fusion_nDCG@5': result['Fusion_nDCG@5']
            }
            ndcg_data.append(row)
        
        with open('results/nDCG.csv', 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=ndcg_data[0].keys())
            writer.writeheader()
            writer.writerows(ndcg_data)
        
        # ä¿å­˜ç›¸å…³æ€§åˆ¤æ–­
        relevance_data = []
        for query_id, judgments in self.relevance_judgments.items():
            for doc_id, relevance in judgments.items():
                row = {
                    'Query_ID': query_id,
                    'Document_ID': doc_id,
                    'Document_Name': self.irs.document_names[doc_id] if doc_id < len(self.irs.document_names) else f'doc_{doc_id}',
                    'Relevance_Score': relevance
                }
                relevance_data.append(row)
        
        with open('results/relevance_judgments.csv', 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=relevance_data[0].keys())
            writer.writeheader()
            writer.writerows(relevance_data)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜:")
        print(f"   â€¢ precision.csv")
        print(f"   â€¢ recall.csv") 
        print(f"   â€¢ nDCG.csv")
        print(f"   â€¢ relevance_judgments.csv")


def main():
    """è¿è¡Œæ”¹è¿›çš„æ–‡æ¡£åŸºç¡€è¯„ä¼°"""
    evaluator = DocumentBasedEvaluator()
    
    if evaluator.run_document_based_evaluation():
        print(f"\nğŸ‰ æ”¹è¿›è¯„ä¼°å®Œæˆ!")
        print(f"ğŸ”¬ ç§‘å­¦ä¸¥è°¨çš„ç®—æ³•è¯„ä¼°")
        print(f"ğŸ“ˆ æ¶ˆé™¤äººä¸ºåå‘çš„å®¢è§‚ç»“æœ")
        print(f"ğŸ“‚ è¯¾ç¨‹ä½œä¸šæ–‡ä»¶å·²å‡†å¤‡å°±ç»ª")
    else:
        print(f"\nâŒ è¯„ä¼°å¤±è´¥!")


if __name__ == "__main__":
    main()